{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WiegerScheurer/rsa_neuroai/blob/main/Tutorial2RSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuro-AI: Representational Similarity Analysis\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jSi9i1FM8b2P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dVgdnsig_ce"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "> By Giacomo Aldegheri, Wieger Scheurer and Julio Smidi\n",
        "\n",
        "\n",
        "In the first part of this tutorial, we will look at *representational similarity analysis* (RSA, [original paper](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)). RSA is a commonly used way to explore and analyse how different stimuli are represented. A big advantage of RSA is that it can compare these representations across different modalities of measurement (for example fMRI, EEG or behavioral responses) and even across subjects and species (humans, monkeys and also neural networks!).\n",
        "\n",
        "For RSA we create a *representational (dis)similarity matrix* (RDM) for each entity, which are made by taking a (dis)similarity measure for the activity patterns between each stimulus pair. One often used measure for this is the correlation distance. With such an RDM, we can now directly compare this to any other RDM, to explore the representational structure differences!\n",
        "\n",
        "In this tutorial, we will again use fMRI data from the 2023 Algonauts challenge. It is a subset of the Natural Scenes Dataset (NSD) only including the visual system. The full dataset includes ~30,000 images per subject, but here, to make computations quicker, we will only use 872 images that were seen by all subjects.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/rsa_overview.jpeg\" alt=\"General RSA overview\" width=\"1000\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is heavily indebted to the following resources:\n",
        "\n",
        "- [Algonauts 2023 Challenge tutorial](https://colab.research.google.com/drive/1bLJGP3bAo_hAOwZPHpiSHKlt97X9xsUw?usp=share_link#scrollTo=gjQrI9AlzDqG)\n",
        "- [Deep NSD tutorial by Colin Conwell](https://colab.research.google.com/drive/1OalDuiQ6Dwg39XT-BkPMz2XAduPUxQtv?usp=sharing)\n",
        "- Relating DCNN features with brain activity tutorial by Jessica Loke\n",
        "- [Comparing brains and DNNs presentation by Martin Hebart](http://algonauts.csail.mit.edu/slides/Martin_Hebart-Comparing_Brains_and_DNNs.pdf)\n",
        "- [RSA toolbox by the RSAgroup](https://rsatoolbox.readthedocs.io/en/latest/overview.html)\n",
        "- [Ni-edu by Lukas Snoek](https://lukas-snoek.com/NI-edu/fMRI-pattern-analysis/week_3/rsa.html#categorical-rdms)\n",
        "- [Voxelwise encoding tutorials by Matteo Visconti di Oleggio Castello (Gallant lab)](https://github.com/gallantlab/voxelwise_tutorials)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vKV5xtL8rG_"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhybE3pajXf3"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnxvl71zARIu"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ElcHg-C8jyV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "from collections import OrderedDict\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from nilearn import datasets, plotting\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import MDS\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from scipy.stats import pearsonr, spearmanr, rankdata\n",
        "from scipy.spatial.distance import cosine\n",
        "import random\n",
        "import glob\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: uncomment if you want to use OpenAI's CLIP model:\n",
        "#!pip install git+https://github.com/openai/CLIP.git\n",
        "#import clip"
      ],
      "metadata": {
        "id": "P60jEu26ybpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_v8BLTqRcn"
      },
      "source": [
        "## Quick notebook execution\n",
        "It is possible that you will have to restart the notebook and execute it from scratch (e.g. sometimes the Runtime gets disconnected). Toggle this to skip execution of computationally intensive cells (e.g. 3D plots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OA5gFo46qQwh"
      },
      "outputs": [],
      "source": [
        "quick_execution = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlO9eCHpF3yb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bGj5NI3ILB9"
      },
      "source": [
        "## Set device\n",
        "If you have the possibility, using the GPU (CUDA) runtime will make computations significantly faster. In Colab, you can press the arrow in the top-right next to the RAM and Disk icons, select \"Change runtime type\" and select the T4 GPU. In the free version of Colab the usage of this is limited however."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UUM-Y0a2-K1E"
      },
      "outputs": [],
      "source": [
        "device = 'cpu' #@param ['cpu', 'cuda'] {allow-input: true}\n",
        "if device=='cuda':\n",
        "    assert torch.cuda.is_available()\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfxTP84Z9PGR"
      },
      "source": [
        "## Get data\n",
        "\n",
        "First, you need to access the data from [this public folder](https://drive.google.com/drive/folders/1AjDOejWLjfXGkr-hK07SZJ_4ni1nypjw?usp=sharing). Before running this tutorial, you need to select the folder and choose \"Add a shortcut to Drive\". This will create a shortcut (without copying or taking space) of the folder to a desired path in your Google Drive, from which you will be able to access the content.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15TNjV__sWCcnBRlxbXNbJfpidx-C6nrk' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iP3tkB6C86TN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "main_dir = '/content/drive/MyDrive/UvA_encodingtutorial' #@param {type:\"string\"}\n",
        "fmri_dir = os.path.join(main_dir, 'fmri_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Jsiqmy9jpH"
      },
      "outputs": [],
      "source": [
        "assert os.path.isdir(main_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7llmuw7_92fo"
      },
      "outputs": [],
      "source": [
        "# The folders of all eight subjects should be there\n",
        "os.listdir(fmri_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDA4lf00ixnJ"
      },
      "source": [
        "# Load and visualize the data\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/breinvlam4.png\" width=\"250\"/>\n",
        "\n",
        "The fMRI data consists of two ```.npy``` files:\n",
        "- ```lh_training_fmri.npy```: the left hemisphere (LH) fMRI data.\n",
        "- ```rh_training_fmri.npy```: the right hemisphere (RH) fMRI data.\n",
        "\n",
        "Both files are 2D arrays, where each row is a stimulus image and each column is an fMRI vertex.\n",
        "\n",
        "\n",
        "> For more information on the fMRI responses please check the [README.txt](https://www.google.com/url?q=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F16oLCaDmUBZuT6z_VGKO-qzwidYDE77Sg%2Fview%3Fusp%3Dshare_link) file from the 2023 Algonauts Challenge.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0B1QdJxVA9b"
      },
      "outputs": [],
      "source": [
        "# @title Utilities to plot ROIs and activations\n",
        "\n",
        "def get_roi_mask(roi, hemisphere, subj):\n",
        "\n",
        "  subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "  # Define the ROI class based on the selected ROI\n",
        "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "      roi_class = 'prf-visualrois'\n",
        "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "      roi_class = 'floc-bodies'\n",
        "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "      roi_class = 'floc-faces'\n",
        "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "      roi_class = 'floc-places'\n",
        "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "      roi_class = 'floc-words'\n",
        "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "      roi_class = 'streams'\n",
        "  elif roi == 'all-vertices':\n",
        "      roi_class = roi\n",
        "\n",
        "  # Load the ROI brain surface maps\n",
        "  fsaverage_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "\n",
        "  fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "\n",
        "  if roi != 'all-vertices':\n",
        "    challenge_roi_class_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "      hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "\n",
        "    challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "\n",
        "    roi_map_dir = os.path.join(subj_dir, 'roi_masks',\n",
        "        'mapping_'+roi_class+'.npy')\n",
        "    roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "    # Select the vertices corresponding to the ROI of interest\n",
        "    roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "    challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "    fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "    return challenge_roi, fsaverage_roi\n",
        "\n",
        "  else:\n",
        "    return None, fsaverage_roi_class\n",
        "\n",
        "\n",
        "def plot_surf_map(subj=1, stat_map=None, hemi='left', roi=None, title=None):\n",
        "  \"\"\"\n",
        "  Plot ROI or statistical\n",
        "  \"\"\"\n",
        "  if roi in [None, 'all-vertices']:\n",
        "    roi = 'all-vertices'\n",
        "\n",
        "  challenge_roi, fsaverage_roi = get_roi_mask(roi, hemi, subj)\n",
        "\n",
        "  # Map the fMRI data onto the brain surface map\n",
        "\n",
        "\n",
        "  if stat_map is None:\n",
        "    fsaverage_response = fsaverage_roi\n",
        "    cmap = 'cool'\n",
        "    colorbar = False\n",
        "  else:\n",
        "    fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "    if roi != 'all-vertices':\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        stat_map[np.where(challenge_roi)[0]]\n",
        "    else:\n",
        "      fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        stat_map\n",
        "    cmap = 'cold_hot'\n",
        "    colorbar = True\n",
        "\n",
        "  if title is None:\n",
        "    title = roi+', '+hemi+' hemisphere'\n",
        "\n",
        "  # Create the interactive brain surface map\n",
        "  fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "  view = plotting.view_surf(\n",
        "      surf_mesh=fsaverage['infl_'+hemi],\n",
        "      surf_map=fsaverage_response,\n",
        "      bg_map=fsaverage['sulc_'+hemi],\n",
        "      threshold=1e-14,\n",
        "      cmap=cmap,\n",
        "      colorbar=colorbar,\n",
        "      title=title\n",
        "      )\n",
        "  return view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eidZIzQBnWKV"
      },
      "outputs": [],
      "source": [
        "# @title Choose subject to visualize\n",
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
        "subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('LH fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Stimulus images × LH vertices)')\n",
        "\n",
        "print('\\nRH fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Stimulus images × RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5Lq8CZtXluMl"
      },
      "outputs": [],
      "source": [
        "# @title Visualize all vertices on a brain surface map\n",
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param ['left', 'right']\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaau-XOvkPuB"
      },
      "outputs": [],
      "source": [
        "# @title Visualize a chosen ROI on the surface\n",
        "if not quick_execution:\n",
        "  hemisphere = 'left' #@param [\"left\", \"right\"]\n",
        "  roi = \"ventral\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
        "  view = plot_surf_map(subj=subj, hemi=hemisphere, roi=roi)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRUsvPO_A12z"
      },
      "source": [
        "### Stimulus images\n",
        "\n",
        "All images come from the [COCO dataset](https://cocodataset.org/#home) of natural scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUfYsTAp_Vdj"
      },
      "outputs": [],
      "source": [
        "stim_dir = os.path.join(main_dir, 'stimuli')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "img_list = os.listdir(stim_dir)\n",
        "img_list.sort()\n",
        "print('Total n. of images: ' + str(len(img_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RIZIvVsvA_aN"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the fMRI response to selected images\n",
        "if not quick_execution:\n",
        "  img = 0 #@param\n",
        "\n",
        "  #Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Image: ' + str(img));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere)\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KKGlDpTXBBsr"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the fMRI responses of a chosen ROI\n",
        "if not quick_execution:\n",
        "\n",
        "  img = 0 #@param\n",
        "  hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
        "  roi = \"ventral\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "  # Load the image\n",
        "  img_file = os.path.join(stim_dir, img_list[img])\n",
        "  this_img = Image.open(img_file).convert('RGB')\n",
        "\n",
        "  # Plot the image\n",
        "  plt.figure()\n",
        "  plt.axis('off')\n",
        "  plt.imshow(this_img)\n",
        "  plt.title('Training image: ' + str(img+1));\n",
        "\n",
        "  stat_map = lh_fmri[img] if hemisphere == 'left' else rh_fmri[img]\n",
        "\n",
        "  view = plot_surf_map(stat_map=stat_map, subj=subj, hemi=hemisphere, roi=roi)\n",
        "\n",
        "else:\n",
        "  view = 'Skipping plot (untoggle quick execution to show plot)'\n",
        "\n",
        "view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiruedt5_WNs"
      },
      "source": [
        "# Feature extraction\n",
        "\n",
        "\n",
        "\n",
        "Here, we're going to extract the feature representations of our stimulus images across different layers of a Deep Convolutional Neural Network (DCNN). These networks process information in a hierarchical fashion, extracting different types of features from the input data. Early DCNN layers extract more low-level visual features (edges, shapes, simple textures), while later layers extract more high-level features (complex patterns, full objects). We can plot these so-called *featuremaps* to inspect how the input is represented across different hierarchical levels of the network:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/cnn_layers.png\" width=\"250\"/>\n",
        "\n",
        "This progressive processing of visual abstraction is to some extent analogous to how bottom-up visual sensory information is processed in early visual cortex. Hence, they are well-suited for comparisons with neural activity evoked by these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiJ_F_agBkUm"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to obtain a model and extract its activations\n",
        "\n",
        "def get_vision_model(whichmodel):\n",
        "  \"\"\"\n",
        "  Get computer vision model. This function can be\n",
        "  modified to return a custom model.\n",
        "  \"\"\"\n",
        "  if whichmodel in ['alexnet', 'resnet50']:\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', whichmodel)\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
        "      transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
        "    ])\n",
        "  elif whichmodel=='clip':\n",
        "    model, preprocess = clip.load(\"ViT-B/32\")\n",
        "  else:\n",
        "    raise ValueError(f'Model {whichmodel} unknown!')\n",
        "\n",
        "  return model, preprocess\n",
        "\n",
        "\n",
        "def list_layers(model):\n",
        "    \"\"\"\n",
        "    List all layers of the model with their names.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for name, mod in model.named_modules():\n",
        "        layers.append(name)\n",
        "    return layers\n",
        "\n",
        "\n",
        "def register_hooks(model, target_layers):\n",
        "    \"\"\"\n",
        "    Register hooks to extract features from specified layers.\n",
        "    \"\"\"\n",
        "    if not isinstance(target_layers, list):\n",
        "      target_layers = [target_layers]\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            features[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name, layer in model.named_modules():\n",
        "        if name in target_layers:\n",
        "            layer.register_forward_hook(get_hook(name))\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvJoJAcVDbsd"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor():\n",
        "    def __init__(self, modelname, device=device, target_layers=None):\n",
        "        self.modelname = modelname\n",
        "        self.device = device\n",
        "        model, self.preprocess = get_vision_model(modelname)\n",
        "        self.model = model.eval().to(self.device)\n",
        "        if target_layers is not None:\n",
        "            self.create_feature_extractor(target_layers)\n",
        "\n",
        "    def create_feature_extractor(self, target_layers):\n",
        "        self.target_layers = target_layers\n",
        "        self.features = register_hooks(self.model, self.target_layers)\n",
        "\n",
        "    def list_layers(self):\n",
        "        return list_layers(self.model)\n",
        "\n",
        "    def run_model(self, x):\n",
        "\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = self.model.encode_image(x) if self.modelname=='clip' else self.model(x)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.run_model(x)\n",
        "        # features = [torch.flatten(l, start_dim=1) for l in self.features.values()]\n",
        "        # features_dict = {}\n",
        "        # for n, l in zip(self.target_layers, self.features.values()):\n",
        "        #     features_dict[n] = torch.flatten(l, start_dim=1).detach().cpu().numpy()\n",
        "        return self.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSx9am5RDxv-"
      },
      "source": [
        "## Create the feature extractor for a specified model and layer(s).\n",
        "\n",
        "Please note that ```target_layers``` can be a list of layers as well. To find out what layers are available for a given model, you can do the following:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Note that we don't pass the 'target_layers'\n",
        "# as a parameter to add them later!\n",
        "feat_extractor = FeatureExtractor(modelname, device=device)\n",
        "\n",
        "feat_extractor.list_layers()\n",
        "```\n",
        " After choosing one or multiple layers from the list, you can do:\n",
        "\n",
        "```\n",
        "feat_extractor.create_feature_extractor(target_layers)\n",
        "```\n",
        "\n",
        "Up to you to experiment and try different layers and models! You can search the literature for ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu4rEycFAODz"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "For some research, we might be more interested in a specific range of DCNN layers than in others. Can you think of a situation in which we'd only want to look at earlier layers of a DCNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzX8TJcAEoH"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzFj-d2wmrZt"
      },
      "source": [
        "### Choose model and layer\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/DCNN_better.png\" width=\"350\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9AN4n5VEANA"
      },
      "outputs": [],
      "source": [
        "modelname = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = ['features.2'] #@param {type: 'string'}\n",
        "\n",
        "# Note that target_layers can be a list of strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2ukKULEDumX"
      },
      "outputs": [],
      "source": [
        "# Create the feature extractor\n",
        "feat_extractor = FeatureExtractor(modelname, device=device, target_layers=target_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKKP4SK7mfWe"
      },
      "source": [
        "### Feature extraction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJfsLtAoFO1R"
      },
      "outputs": [],
      "source": [
        "def extract_features(feature_extractor, dataloader):\n",
        "    features = []\n",
        "    features_dict = {layer: [] for layer in feature_extractor.target_layers}\n",
        "\n",
        "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        # Extract features\n",
        "        ft = feature_extractor(d)\n",
        "        ft_stacked = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
        "        features.append(ft_stacked.detach().cpu().numpy())\n",
        "\n",
        "        # Append features for each layer separately\n",
        "        for layer, layer_features in ft.items():\n",
        "            # Flatten the features\n",
        "            flattened_features = torch.flatten(layer_features, start_dim=1)\n",
        "            features_dict[layer].append(flattened_features.detach().cpu().numpy())\n",
        "\n",
        "    features_dict = {layer: np.vstack(layer_features) for layer, layer_features in features_dict.items()}\n",
        "\n",
        "    return np.vstack(features), features_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBzGFYiDA8J"
      },
      "source": [
        "## Create image dataset\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/nsd_stimuli2.jpeg\" width=\"350\"/>\n",
        "\n",
        "Here we create a class instance that allows us to load the stimulus images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kybLxlfRCwuA"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Pytorch dataset that loads the images\n",
        "    from our stimulus set.\n",
        "    \"\"\"\n",
        "    def __init__(self, imgs_paths, idxs, transform, device=device):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(self.device)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OslrQHsnaYm"
      },
      "source": [
        "### Split fMRI data into training and validation partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKSt3UBouCs"
      },
      "source": [
        "For classical RSA, we actually don't need to split our data into training and validation sets; after all, we are only correlating the data, so there is no regression of any kind happening. However, later we are going to do a reweighted RSA (rRSA), and for this we do need a training/validation split. For ease we therefore opt to do the split here already. This way we can also compare the classical RSA with the rRSA on the same amount of stimuli.\n",
        "\n",
        "Feel free however to do the RSA with the training and validation sets together, you just need to append the fMRI and DCNN feature training and validation sets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GrSXp6go74O"
      },
      "outputs": [],
      "source": [
        "rand_seed = 123 #@param\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "train_test_split = 90 # @param\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(img_list) / 100 * train_test_split))\n",
        "# Shuffle all stimulus images\n",
        "idxs = np.arange(len(img_list))\n",
        "np.random.shuffle(idxs)\n",
        "\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_test = idxs[:num_train], idxs[num_train:]\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcXTCyHNDJ1X"
      },
      "outputs": [],
      "source": [
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_test = lh_fmri[idxs_test]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_test = rh_fmri[idxs_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3le_qcebDIfc"
      },
      "outputs": [],
      "source": [
        "batch_size = 100 #@param\n",
        "# Get the paths of all image files\n",
        "imgs_paths = sorted(list(Path(stim_dir).iterdir()))\n",
        "\n",
        "# The DataLoaders contain the ImageDataset class\n",
        "train_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_train, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(imgs_paths, idxs_test, transform=feat_extractor.preprocess),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqShGYpeogRW"
      },
      "source": [
        "## Extract features\n",
        "\n",
        "Now, we can extract the feature space representations of our images from the chosen DCNN. The resulting matrices provide an insight into how the visual information is represented at different hierarchical levels of the neural network.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **Beware:** this can take a while (+-10 min), depending on the chosen model and layer to extract features from.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In the meantime it can be worth briefly scanning over the following papers on brain alignment of DCNNs:\n",
        "\n",
        "*Yamins & DiCarlo (2016): Using goal-driven deep learning models to understand\n",
        "sensory cortex*\n",
        "\n",
        "https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec15/YaminsNature2016.pdf\n",
        "\n",
        "*Güçlü & van Gerven (2015): Deep Neural Networks Reveal a Gradient in the Complexity\n",
        "of Neural Representations across the Ventral Stream*\n",
        "\n",
        "https://www.jneurosci.org/content/jneuro/35/27/10005.full.pdf?ref=https://githubhelp.com\n",
        "\n",
        "*Kriegeskorte (2015): Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing*\n",
        "\n",
        "https://www.annualreviews.org/content/journals/10.1146/annurev-vision-082114-035447\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otYK3oizDjE6"
      },
      "outputs": [],
      "source": [
        "features_train = extract_features(feat_extractor, train_imgs_dataloader)[0]\n",
        "features_test = extract_features(feat_extractor, test_imgs_dataloader)[0]\n",
        "\n",
        "print('\\nTraining images features:')\n",
        "print(features_train.shape)\n",
        "print('(Training stimulus images × features)')\n",
        "\n",
        "print('\\nValidation images features:')\n",
        "print(features_test.shape)\n",
        "print('(Validation stimulus images × features)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yBI3hRVIRDm"
      },
      "source": [
        "# RSA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hewt4ZVpIYlH"
      },
      "source": [
        "## Constructing RDMs\n",
        "\n",
        "Now that we have extracted the DCNN features, we can use those to compute *representational dissimilarity matrices* (RDMs). Differences in feature representations between stimuli can be compared, giving us a matrix that has a (dis)similarity score between each stimulus pair (and hence a diagonal of zeros). An advantage of these RDMs is that once constructed, they can easily be compared across modalities (e.g. DNN and fMRI response RDMs), regardless of the feature space in which input is provided. This invariance between feature spaces is one of the main reasons why RSA is such a useful method. It allows us to compare modalities that are measured in very distinct ways, producing very different types of data. RDMs eliminate the restrictions imposed by inherent differences between modality-specific data by transforming data into a new space that captures the underlying **representational geometry**.\n",
        "\n",
        "\n",
        "Our data is currently of the shape (stimulus images x fMRI/DCNN features), but will take on the shape (stimulus images x stimulus images) once we compute our RDMs. Each column in our current data tells us how the corresponding image is represented when proccesed by the brain (in voxel activations --> fMRI features) or by a DCNN (in feature maps). When transforming our modality-specific features into RDMs, we are essentially looking at the pairwise differences in stimulus representation.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/WiegerScheurer/rsa_neuroai/main/rdm.png\" width=\"400\"/>\n",
        "\n",
        "Because these are relative differences, RSA is indifferent to the original feature space of our data. You could think of this as comparing the relation between  word pairs in different languages, expressed through distances between word vectors in embedding space. Similariy, we want to retain only the geometrical information encapsulated in the original feature space. To do so, we compute the pairwise distances between how stimuli are represented. We do this for every combination of stimuli, producing our stimuli x stimuli shaped RDM. Each element in this matrix thus represents how different those two stimuli are represented in their original feature space. Consequently, we can compare these representational geometric patterns across modalities.\n",
        "\n",
        "---\n",
        "####**Question:**\n",
        "Can you think of data that RSA allows us to compare, previously either impractical or simply impossible?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER HERE**"
      ],
      "metadata": {
        "id": "2i0oB4X754bF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCr7yNdlGmbg"
      },
      "outputs": [],
      "source": [
        "def create_rdm(activations):\n",
        "    \"\"\"\n",
        "    Function to create an RDM from response activations or features.\n",
        "    \"\"\"\n",
        "    rdm = np.corrcoef(activations)\n",
        "    return rdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydFi53UJKnAJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create Subject RDM\n",
        "rdm_lh_train = create_rdm(lh_fmri_train)\n",
        "rdm_lh_test = create_rdm(lh_fmri_test)\n",
        "rdm_rh_train = create_rdm(rh_fmri_train)\n",
        "rdm_rh_test = create_rdm(rh_fmri_test)\n",
        "\n",
        "print(\"\\nTraining images left and right hemisphere RDM\")\n",
        "print(rdm_lh_train.shape)\n",
        "print(rdm_rh_train.shape)\n",
        "print('(Training stimulus images × Training stimulus images)')\n",
        "\n",
        "print(\"\\nValidation images left and right hemishpere RDM\")\n",
        "print(rdm_lh_test.shape)\n",
        "print(rdm_rh_test.shape)\n",
        "print('(Validation stimulus images × Validation stimulus images)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1VLa36gLaTS"
      },
      "outputs": [],
      "source": [
        "# @title Plot subject RDM\n",
        "plot_n_stimuli = 100 # @param\n",
        "hemisphere = \"left\" # @param\n",
        "sns.heatmap(rdm_lh_train[:plot_n_stimuli,:plot_n_stimuli])\n",
        "plt.title(f\"RDM of first {plot_n_stimuli} training stimuli of the {hemisphere} hemisphere\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFafEwKzKbHB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create DNN RDM\n",
        "rdm_dnn_train = create_rdm(features_train)\n",
        "rdm_dnn_val = create_rdm(features_test)\n",
        "\n",
        "print(\"\\nTraining images DNN RDM\")\n",
        "print(rdm_dnn_train.shape)\n",
        "print('(Training stimulus images × Training stimulus images)')\n",
        "\n",
        "print(\"\\nValidation images DNN RDM\")\n",
        "print(rdm_dnn_val.shape)\n",
        "print('(Validation stimulus images × Validation stimulus images)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NoHkIphuLL6O"
      },
      "outputs": [],
      "source": [
        "# @title Plot DNN RDM\n",
        "sns.heatmap(rdm_dnn_train[:plot_n_stimuli,:plot_n_stimuli])\n",
        "plt.title(f\"RDM of first {plot_n_stimuli} training stimuli of {modelname} {target_layers}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY09rieVNc4C"
      },
      "source": [
        "## Compare RDMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQAJ_c7Nxp-"
      },
      "source": [
        "As the RDMs are symmetric along the diagonal, we only need either of the sides (we take the upper triangle). Otherwise, the comparison value we find will be higher than the true correlation. We then flatten this upper triangle to obtain a *representational dissimilarity vector* (RDV). This can conveniently be done using the `triu_indices` function from NumPy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTAg260OGvF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to vectorize a matrix\n",
        "def triu_vector(rdm):\n",
        "    \"\"\"\n",
        "    Function to extract the upper triangle from a matrix.\n",
        "    \"\"\"\n",
        "    n_stimuli = rdm.shape[0]\n",
        "    triangle = np.triu_indices(n_stimuli, k=1)\n",
        "    rdv = rdm[triangle]\n",
        "\n",
        "    return rdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_np69cIFVUZ"
      },
      "source": [
        "## Comparison methods\n",
        "Now, all that's left is comparing how these representations of our stimulus images relate to one another. We can compare our RDVs by computing how similar they are using similarity metrics. There are various ways of doing so, depending on different facets of your data and research question. When deciding what similarity metric to use, you should take into account:\n",
        "\n",
        "- **Variance in data scale:** Does the data underlying your RDM/Vs exist on very different scales? if so, do you need to take these differences in scale into account?\n",
        "- **Linearity of relation:** Is it reasonable to expect a linear relationship between your representations?\n",
        "- **Outlier leniency:** How to deal with outliers?\n",
        "- **Interpretability of results:** Not all similarity metrics are as interpretable. Ideally your metric of choice exhibits a local optimal balance between complexity and interpretability.\n",
        "\n",
        "However, the most common and stringent option is computing the cosine similarity between your RDVs. Other options include Pearson's correlation or Spearman's rank correlation without tie correction known as Spearman's $ρ_α$. [Source](https://rsatoolbox.readthedocs.io/en/latest/comparing.html)\n",
        "\n",
        "---\n",
        "####**Question:**\n",
        "Come up with 2 research questions. One for which your comparisons are indifferent to variance in data scale, and one for which you should account for potential differences in data scale. Which similarity metric is best in each situation?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER HERE**"
      ],
      "metadata": {
        "id": "7WK3CdBR6T4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Question:**\n",
        "Try to implement the cosine distance comparison, and one of the correlation methods. For beginners we advise doing the Pearson's correlation, otherwise Spearman's $\\rho_\\alpha$ is more challenging. For an extra challenge, try to do this wihtout using any of the Sklearn or SciPy correlation functions. You can make use of the `triu_vector` helper function."
      ],
      "metadata": {
        "id": "-n7mVv7jZ1bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity is defined as:\n",
        "\n",
        "\n",
        "$$\\text{similarity}(X, Y) = \\frac{\\sum_{i=1}^{n} X_i Y_i}{\\sqrt{\\sum_{i=1}^{n} X_i^2 Y_i^2}}$$\n",
        "\n",
        "Where $X$ and $Y$ are the vectorized RDMs."
      ],
      "metadata": {
        "id": "V_c2zlEte97J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl_X0GFPLs4d"
      },
      "outputs": [],
      "source": [
        "def compare_cosine(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes the cosine similarity between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Click to see solution\n",
        "def compare_cosine(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes the cosine similarity between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    covariance = np.sum(np.dot(rdv1,rdv2))\n",
        "\n",
        "    std_dev1 = np.sqrt(np.sum(rdv1**2))\n",
        "    std_dev2 = np.sqrt(np.sum(rdv2**2))\n",
        "\n",
        "    correlations = covariance / np.dot(std_dev1, std_dev2)\n",
        "\n",
        "    return correlations"
      ],
      "metadata": {
        "cellView": "form",
        "id": "br5vjFWbcChD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson's correlation is defined as:\n",
        "\n",
        "$$r = \\frac{\\sum_{i=1}^{n} (X_i -\\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i -\\bar{X})^2(Y_i -\\bar{Y})^2}}$$\n",
        "\n",
        "Where $X$ and $Y$ are the vectorized RDMs."
      ],
      "metadata": {
        "id": "JMq6xs0dbak0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrtgDjbZNVIY"
      },
      "outputs": [],
      "source": [
        "def compare_pearson(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and returns Pearson's correlation between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Click to see solution\n",
        "def compare_pearson(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and returns Pearson's correlation between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    rdv1 = rdv1 - np.mean(rdv1)\n",
        "    rdv2 = rdv2 - np.mean(rdv2)\n",
        "\n",
        "    covariance = np.sum(rdv1 * rdv2)\n",
        "\n",
        "    std_dev1 = np.sqrt(np.sum(rdv1**2))\n",
        "    std_dev2 = np.sqrt(np.sum(rdv2**2))\n",
        "\n",
        "    correlations = covariance / (std_dev1 * std_dev2)\n",
        "\n",
        "    return correlations"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X1wdWViCeY_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spearman's $\\rho_\\alpha$ is given by:\n",
        "\n",
        "$$\\frac{12}{n^3 - n}\\sum_{i=1}^{n} (R_{X_i} - \\bar{R}_X) (R_{Y_i} - \\bar{R}_Y)$$\n",
        "\n",
        "Where $R_{X_i}$ and R_{Y_i} are the ranked vectorized RDMs."
      ],
      "metadata": {
        "id": "AfOOGkZVbiLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2lSmU8GM0ja"
      },
      "outputs": [],
      "source": [
        "def compare_spearman_rho_alpha(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes Spearman's rho_alpha between the two.\n",
        "    \"\"\"\n",
        "    # Your code here!\n",
        "\n",
        "    return correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Click to see solution\n",
        "def compare_spearman_rho_alpha(rdm1, rdm2):\n",
        "    \"\"\"\n",
        "    Takes two RDMs and computes Spearman's rho_alpha between the two.\n",
        "    \"\"\"\n",
        "    rdv1 = triu_vector(rdm1)\n",
        "    rdv2 = triu_vector(rdm2)\n",
        "\n",
        "    rdv1 = rankdata(rdv1)\n",
        "    rdv2 = rankdata(rdv2)\n",
        "\n",
        "    rdv1 = rdv1 - np.mean(rdv1)\n",
        "    rdv2 = rdv2 - np.mean(rdv2)\n",
        "\n",
        "    n_stimuli = rdv1.shape[0]\n",
        "    correlation = 12 * np.dot(rdv1, rdv2) / (n_stimuli ** 3 - n_stimuli)\n",
        "\n",
        "    return correlation"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SEwlWl-xbh58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNca_vP9cGhR"
      },
      "outputs": [],
      "source": [
        "def compare_rdms(rdm1, rdm2, method=\"rho-alpha\"):\n",
        "    if method == 'cosine':\n",
        "        sim = compare_cosine(rdm1, rdm2)\n",
        "    elif method == 'pearson':\n",
        "        sim = compare_pearson(rdm1, rdm2)\n",
        "    elif method == 'rho-alpha':\n",
        "        sim = compare_spearman_rho_alpha(rdm1, rdm2)\n",
        "    else:\n",
        "        raise ValueError('Unknown comparison method requested!')\n",
        "    return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYUoUYbZJP_X",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Choose comparison method and RDMs\n",
        "method = \"rho-alpha\" # @param [\"cosine\", \"pearson\", \"spearman\", \"rho-alpha\"]\n",
        "rdm1 = rdm_lh_train # @param [\"rdm_lh_train\", \"rdm_rh_train\"]\n",
        "rdm2 = rdm_dnn_train # @param [\"rdm_dnn_train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xL8X8zbg6uX"
      },
      "source": [
        "Now we can use the `compare_rdms` function to compare 2 RDMs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS0Voe6Fbwq0"
      },
      "outputs": [],
      "source": [
        "correlation = compare_rdms(rdm1, rdm2, method)\n",
        "print(f\"The {method} correlation is {correlation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dASHo4JCqAOK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions to get ROI and DNN RDMs\n",
        "def get_roi_rdms(subj, roi):\n",
        "\n",
        "    subj_dir = os.path.join(fmri_dir, f'subj{subj:02d}')\n",
        "\n",
        "    lh_fmri = np.load(os.path.join(subj_dir, 'lh_training_fmri.npy'))\n",
        "    rh_fmri = np.load(os.path.join(subj_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "    lh_fmri_train = lh_fmri[idxs_train]\n",
        "    lh_fmri_test = lh_fmri[idxs_test]\n",
        "    rh_fmri_train = rh_fmri[idxs_train]\n",
        "    rh_fmri_test = rh_fmri[idxs_test]\n",
        "\n",
        "    # Get subject ROI data\n",
        "    lh_mask = get_roi_mask(roi, 'lh', subj)[0][:lh_fmri.shape[1]]\n",
        "    lh_roi_train = lh_fmri_train[:,np.where(lh_mask)[0]]\n",
        "    lh_roi_test = lh_fmri_test[:,np.where(lh_mask)[0]]\n",
        "\n",
        "    rh_mask = get_roi_mask(roi, 'rh', subj)[0][:lh_fmri.shape[1]]\n",
        "    rh_roi_train = rh_fmri_train[:,np.where(rh_mask)[0]]\n",
        "    rh_roi_test = rh_fmri_test[:,np.where(rh_mask)[0]]\n",
        "\n",
        "    # Create ROI RDMs\n",
        "    rdm_lh_roi_train = create_rdm(lh_roi_train)\n",
        "    rdm_lh_roi_test = create_rdm(lh_roi_test)\n",
        "    rdm_rh_roi_train = create_rdm(rh_roi_train)\n",
        "    rdm_rh_roi_test = create_rdm(rh_roi_test)\n",
        "\n",
        "    return rdm_lh_roi_train, rdm_lh_roi_test, rdm_rh_roi_train, rdm_rh_roi_test\n",
        "\n",
        "def get_dnn_rdms(model, layers):\n",
        "    feat_extractor = FeatureExtractor(model, device=device, target_layers=layers)\n",
        "\n",
        "    _, features_train_dict = extract_features(feat_extractor, train_imgs_dataloader)\n",
        "    _, features_test_dict = extract_features(feat_extractor, test_imgs_dataloader)\n",
        "\n",
        "    rdm_dnn_train = np.zeros((len(idxs_train), len(idxs_train), len(layers)))\n",
        "    rdm_dnn_test = np.zeros((len(idxs_test), len(idxs_test), len(layers)))\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        rdm_dnn_train[:,:,i] = create_rdm(features_train_dict[layer])\n",
        "        rdm_dnn_test[:,:,i] = create_rdm(features_test_dict[layer])\n",
        "\n",
        "    return rdm_dnn_train, rdm_dnn_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mc_W42Ao0Hu"
      },
      "outputs": [],
      "source": [
        "lh_train, lh_test, rh_train, rh_test = get_roi_rdms(1, \"ventral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL_zryFbp099"
      },
      "outputs": [],
      "source": [
        "compare_rdms(lh_train, rh_train, method=\"rho-alpha\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTL4vVHU7WHP"
      },
      "source": [
        "## Compare multiple layers and ROIs\n",
        "\n",
        "Now it is time to explore the true flexibility of RSA. We can make comparisons using different DNN layers and ROIs. As long as the RDMs are of the same shape, virtually any modality could be used in the comparison!\n",
        "\n",
        "We will here choose one model and create and RDM for multiple layers of this model. We will then correlate each of these RDMs to both the \"early\" and \"ventral\" ROIs, similar to the first tutorial.\n",
        "\n",
        "Feel free to experiment with different DNNs, layers and ROIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QElW5AzJK36",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Choose model and layers\n",
        "modelname = 'alexnet' #@param ['alexnet', 'resnet50', 'clip'] {allow-input: true}\n",
        "target_layers = [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\"] #@param {type: 'string'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOmU1_aV7u-d"
      },
      "outputs": [],
      "source": [
        "rdm_dnn_train, rdm_dnn_test = get_dnn_rdms(modelname, target_layers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions to plot RSA results\n",
        "def barplot(ax, data, x, y, hue, title=None, noise_ceilings=None, ylim=None):\n",
        "    \"\"\"\n",
        "    Function to show a barplot with an additional (optional) noise ceiling.\n",
        "    \"\"\"\n",
        "    sns.barplot(ax=ax, x=x, hue=hue, y=y, errorbar=('ci', 68), data=data)\n",
        "\n",
        "    if ylim is not None:\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    if noise_ceilings is not None:\n",
        "        upper_lim = max([nc[1] for nc in noise_ceilings]) + 0.1\n",
        "        ax.set_ylim(0, upper_lim)\n",
        "\n",
        "        bar_width = [b.get_width() for b in ax.patches if b.get_width() != 0][0]\n",
        "\n",
        "        noiseceil_idx = 0\n",
        "\n",
        "        for i, roi in enumerate(data['roi'].unique()):\n",
        "\n",
        "            nc1 = noise_ceilings[noiseceil_idx]\n",
        "            left1 = i - bar_width\n",
        "            bottom1 = nc1[0]\n",
        "\n",
        "            ax.add_patch(plt.Rectangle((left1, bottom1), bar_width-0.01, nc1[1] - nc1[0], color='grey', alpha=0.3))\n",
        "\n",
        "            nc2 = noise_ceilings[noiseceil_idx + 1]\n",
        "            left2 = i\n",
        "            bottom2 = nc2[0]\n",
        "\n",
        "            ax.add_patch(plt.Rectangle((left2, bottom2), bar_width-0.01, nc1[1] - nc1[0], color='grey', alpha=0.3))\n",
        "\n",
        "            noiseceil_idx += 2\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "def plot_all_layers(data, x='roi', y='mean_corr', hue='hemisphere', noise_ceilings=None, n_cols=3):\n",
        "    \"\"\"\n",
        "    Function to create barplots for each layer in a single figure with a common y-axis.\n",
        "    \"\"\"\n",
        "    layers = data['layer'].unique()\n",
        "    n_layers = len(layers)\n",
        "    n_rows = (n_layers + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
        "\n",
        "    # Determine the global y-axis limits\n",
        "    global_min = data[y].min()\n",
        "    global_max = data[y].max()\n",
        "    ylim = (global_min, global_max)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows), sharex=True)\n",
        "    axes = axes.flatten()  # Flatten in case we have more than 1 row\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        ax = axes[i]\n",
        "        layer_data = data[data['layer'] == layer]\n",
        "        layer_title = f'Layer: {layer}'\n",
        "\n",
        "        barplot(ax, layer_data, x=x, y=y, hue=hue, title=layer_title, noise_ceilings=noise_ceilings, ylim=ylim)\n",
        "\n",
        "    # Remove any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KOs0aF3DRuQW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX1TV7lEJaQ7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Choose ROIs\n",
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rsa_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "    roi_1_lh_train, _, roi_1_rh_train, _ = get_roi_rdms(subj, roi_1)\n",
        "    roi_2_lh_train, _, roi_2_rh_train, _ = get_roi_rdms(subj, roi_2)\n",
        "\n",
        "    for layer in range(5):\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_1_lh_train),\n",
        "            'hemisphere': 'left',\n",
        "            'roi': roi_1\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_1_rh_train),\n",
        "            'hemisphere': 'right',\n",
        "            'roi': roi_1\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_2_lh_train),\n",
        "            'hemisphere': 'left',\n",
        "            'roi': roi_2\n",
        "        })\n",
        "        rsa_df.append({\n",
        "            'subject': f'subj-{subj:02d}',\n",
        "            'layer': f'{target_layers[layer]}',\n",
        "            f'mean_corr': compare_rdms(rdm_dnn_train[:,:,layer], roi_2_rh_train),\n",
        "            'hemisphere': 'right',\n",
        "            'roi': roi_2\n",
        "        })\n",
        "\n",
        "rsa_df = pd.DataFrame(rsa_df)"
      ],
      "metadata": {
        "id": "bKjoFz1SourQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBYdmDD4Ltsh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot results\n",
        "plot_all_layers(rsa_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise ceiling\n",
        "\n",
        "Similar to yesterday's tutorial, we will calculate the noise ceiling across participants [source](https://www.johancarlin.com/understanding-noise-ceiling-metrics-rsa-compared-to-spearman-brown.html). For RSA, this is relatively straightforward, as we can use the subject RDMs in place of the activations. Recall the explanation from Tutorial 1:\n",
        "\n",
        "- We estimate a *lower noise ceiling* by correlating the activations of a held-out subject to the mean activations of the remaining $N-1$ subjects. This is an under-estimate of the true maximum correlation.\n",
        "\n",
        "- And we estimate an *upper noise ceiling* by correlating the activations of each subject to the mean activations of all subjects, *including* that subject. This is an over-estimate of the true maximum correlation.\n",
        "\n",
        "We estimate both of these across all participants, and take the average. The true noise ceiling will lie somewhere in between these two extremes."
      ],
      "metadata": {
        "id": "sFzVzwaMYNoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_noise_ceiling(rdms):\n",
        "    \"\"\"Returns the lower noise ceiling given a list of subject RDMS.\"\"\"\n",
        "    num_subjs = len(rdms)\n",
        "    lnc = 0.0\n",
        "\n",
        "    for i in range(num_subjs):\n",
        "        subj_rdm = rdms[i]\n",
        "        subj_rdm = subj_rdm[np.triu_indices(subj_rdm.shape[0],k=1)]\n",
        "        rdm_subj_removed = np.delete(rdms, i, axis=0)\n",
        "\n",
        "        mean_subj_rdm = np.mean(rdm_subj_removed, axis=0)\n",
        "        mean_subj_rdm = mean_subj_rdm[np.triu_indices(mean_subj_rdm.shape[0],k=1)]\n",
        "        lnc += pearsonr(subj_rdm, mean_subj_rdm)[0]\n",
        "\n",
        "    lnc = lnc / num_subjs\n",
        "    return lnc"
      ],
      "metadata": {
        "id": "NXcsub9Sianx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upper_noise_ceiling(rdms):\n",
        "    \"\"\"Returns the upper noise ceiling given a list of subject RDMS.\"\"\"\n",
        "    num_subjs = len(rdms)\n",
        "    unc = 0.0\n",
        "    mean_subj_rdm = np.mean(rdms, axis=0)\n",
        "    mean_subj_rdm = mean_subj_rdm[np.triu_indices(mean_subj_rdm.shape[0],k=1)]\n",
        "\n",
        "    for i in range(num_subjs):\n",
        "        subj_rdm = rdms[i]\n",
        "        subj_rdm = subj_rdm[np.triu_indices(subj_rdm.shape[0],k=1)]\n",
        "        unc += pearsonr(subj_rdm, mean_subj_rdm)[0]\n",
        "\n",
        "    unc = unc / num_subjs\n",
        "    return unc"
      ],
      "metadata": {
        "id": "3za3kevViU5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_noise_ceiling(roi, hemi):\n",
        "    \"\"\"Compute the noise ceiling for a ROI and hemisphere.\"\"\"\n",
        "    all_rdms = []\n",
        "    for subj in range(1, 9):\n",
        "        if hemi == \"lh\":\n",
        "            rdm, _, _, _ = get_roi_rdms(subj, roi)\n",
        "        elif hemi == \"rh\":\n",
        "            _, _, rdm, _ = get_roi_rdms(subj, roi)\n",
        "\n",
        "        all_rdms.append(rdm)\n",
        "\n",
        "    lower = lower_noise_ceiling(all_rdms)\n",
        "    upper = upper_noise_ceiling(all_rdms)\n",
        "\n",
        "    return (lower, upper)"
      ],
      "metadata": {
        "id": "U98_D2TxfIxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute the noise ceiling for each ROI and hemisphere\n",
        "all_noiseceilings = []\n",
        "for roi in [roi_1, roi_2]:\n",
        "    for hemi in ['lh', 'rh']:\n",
        "        this_nc = compute_noise_ceiling(roi, hemi)\n",
        "        print(roi, hemi, this_nc)\n",
        "        all_noiseceilings.append(this_nc)"
      ],
      "metadata": {
        "id": "fV7KUpJ1cEXS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot results with noise ceilings\n",
        "plot_all_layers(rsa_df, noise_ceilings=all_noiseceilings)"
      ],
      "metadata": {
        "id": "t8IZ_PbPlecl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bINHNWXmcxeR"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "What conlusions can we draw from these results? What might be possibilities to improve the alignment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF2C7vAOcxeb"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWgkSWA1Qn0n"
      },
      "source": [
        "# Reweighted RSA\n",
        "\n",
        "Classical RSA gives equal value to all features. This might lead to underestimating the actual correlation between the neural data and the DNN activations. One way to improve this alignment is to do a reweighting of the features. [This paper](https://www.sciencedirect.com/science/article/pii/S105381192200413X) provides an in depth explanation of reweighted RSA and its possibilities.\n",
        "\n",
        "We can reweight the features using a regression, a common type of regression to use for this is a ridge regression (there it is again!) with cross-validation for the parameters.\n",
        "To remind you what the ridge optimization problem looks like:\n",
        "\n",
        "$$w = \\arg\\min_{w}||Xw - y||^2+\\alpha||w||^2$$\n",
        "\n",
        "We provide a search space for the `alpha` hyperparameter, but feel free to adjust it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkS9lnJrQnGP"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions for the ridge regression\n",
        "def get_preds_from_RDM(rdm):\n",
        "    '''\n",
        "    This function takes in an RDM and returns a feature matrix\n",
        "    The RDM is in a shape of (n_stimuli, n_stimuli, n_layers)\n",
        "    '''\n",
        "    # Get shape of flattened upper triangle\n",
        "    rdv = triu_vector(rdm[:,:,0])\n",
        "\n",
        "    features = np.zeros((rdv.shape[0], rdm.shape[2])) # Length of flattened upper triangle x number of layers\n",
        "\n",
        "    # Fill in predictors\n",
        "    for l in range(rdm.shape[2]):\n",
        "        features[:,l] = triu_vector(rdm[:,:,l])\n",
        "\n",
        "    # Standardization step\n",
        "    scaler = StandardScaler()\n",
        "    features_s = np.zeros((features.shape))\n",
        "    for l in range(features.shape[1]):\n",
        "        feats_s = scaler.fit_transform(features[:,l].reshape(-1,1))\n",
        "        features_s[:,l] = feats_s[:,0]\n",
        "\n",
        "    return features_s\n",
        "\n",
        "def fit_RIDGE(subj_rdm, dnn_rdm):\n",
        "    \"\"\"Fit a ridge regression to the RDMs\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    alphas = np.logspace(-1, 5, 7)\n",
        "    all_betas = np.zeros(dnn_rdm.shape[1])\n",
        "    rdv = triu_vector(subj_rdm)\n",
        "    rdv = scaler.fit_transform(rdv.reshape(-1,1))\n",
        "    model = RidgeCV(alphas=alphas, scoring='explained_variance').fit(dnn_rdm, rdv)\n",
        "    all_betas = model.coef_\n",
        "\n",
        "    return all_betas\n",
        "\n",
        "def reweighted_rsa(dnn_train, dnn_test, subj_train, subj_test):\n",
        "\n",
        "    feats = get_preds_from_RDM(dnn_train)\n",
        "\n",
        "    # Regression betas\n",
        "    betas = fit_RIDGE(subj_train, feats)[0]\n",
        "\n",
        "    scaler=StandardScaler()\n",
        "    subj_test_rdv = triu_vector(subj_test)\n",
        "    subj_test_rdv = scaler.fit_transform(subj_test_rdv.reshape(-1,1))\n",
        "\n",
        "    pred_rw = get_preds_from_RDM(dnn_test) @ betas\n",
        "\n",
        "    corr = pearsonr(pred_rw.flatten(), subj_test_rdv.flatten())[0]\n",
        "\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose ROIs\n",
        "roi_1 = 'early' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "roi_2 = 'ventral' #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "rrsa_df = []\n",
        "\n",
        "for subj in tqdm(range(1, 9)):\n",
        "    roi_1_lh_train, roi_1_lh_test, roi_1_rh_train, roi_1_rh_test = get_roi_rdms(subj, roi_1)\n",
        "    roi_2_lh_train, roi_2_lh_test, roi_2_rh_train, roi_2_rh_test = get_roi_rdms(subj, roi_2)\n",
        "\n",
        "    lh_rw1 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_1_lh_train, roi_1_lh_test)\n",
        "    rh_rw1 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_1_rh_train, roi_1_rh_test)\n",
        "    lh_rw2 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_2_lh_train, roi_2_lh_test)\n",
        "    rh_rw2 = reweighted_rsa(rdm_dnn_train, rdm_dnn_test, roi_2_rh_train, roi_2_rh_test)\n",
        "\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': lh_rw1,\n",
        "        'hemisphere': 'left',\n",
        "        'roi': roi_1\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': rh_rw1,\n",
        "        'hemisphere': 'right',\n",
        "        'roi': roi_1\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': lh_rw2,\n",
        "        'hemisphere': 'left',\n",
        "        'roi': roi_2\n",
        "    })\n",
        "    rrsa_df.append({\n",
        "        'subject': f'subj-{subj:02d}',\n",
        "        'layer': 'reweighted',\n",
        "        f'mean_corr': rh_rw2,\n",
        "        'hemisphere': 'right',\n",
        "        'roi': roi_2\n",
        "    })\n",
        "\n",
        "rrsa_df = pd.DataFrame(rrsa_df)\n"
      ],
      "metadata": {
        "id": "uManRQm3sNCm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot results with noise ceilings\n",
        "plot_all_layers(rrsa_df, noise_ceilings=all_noiseceilings)"
      ],
      "metadata": {
        "id": "fmDTNvnww6SS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPtbro2VsDTF"
      },
      "source": [
        "---\n",
        "####**Question:**\n",
        "Can you see any improvement compared to the classical RSA? How could this be explained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQA09qeksDTI"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqtGmJ9sT7Sq"
      },
      "source": [
        "# Multidimensional scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IIus7Pqim7"
      },
      "source": [
        "With *multidimensional scaling* (MDS) we can force our RDMs to populate a smaller space (usually two dimensions). This way, we can visualize whether representations of similar images are actually closer together, and thus more similarly represented. This can help us explore te data, potentially revealing patterns we might not have thought of.\n",
        "\n",
        "To not make it to crowded, we use the test set for the visualization. Feel free to change the data used!\n",
        "\n",
        "Although the COCO images themselves are unlabeled, the segmented object in the images are labeled. We can use these labels to filter the images on whether they contain an object or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFA-0zWaKxX1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get object label dataframe\n",
        "df = pd.read_csv(main_dir + \"/additional_data/stimulus_data.csv\")\n",
        "df['nsd_id'] = df['image_name'].str.extract(r'(nsd\\d+)')\n",
        "shared_ids = [id.split('_')[-1].split('.')[0].replace('-', '') for id in img_list]\n",
        "shared_ids = [f\"nsd{int(id[3:]) + 1:05d}\" for id in shared_ids]\n",
        "df = df[df['nsd_id'].isin(shared_ids)]\n",
        "df = df.sort_values(by='nsd_id').reset_index()\n",
        "df = df.loc[sorted(idxs_test)].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG_EVshqX0Rv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Select super category\n",
        "super_category = 'vehicle' # @param ['person', 'sports', 'animal', 'vehicle', 'kitchen', 'appliance', 'electronic', 'food', 'furniture', 'indoor', 'outdoor', 'accessory']\n",
        "object_ids = df.index[df['coco_supercategs'].apply(lambda x: super_category in x)].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGFLkQyDsTJ4"
      },
      "outputs": [],
      "source": [
        "# @title Plot MDS\n",
        "mds = MDS(dissimilarity='precomputed', n_components=2, normalized_stress='auto')\n",
        "mds_R = mds.fit_transform(rdm_dnn_test[:,:,0])\n",
        "print(\"Shape of mds_R:\", mds_R.shape)\n",
        "is_object = np.array([i in object_ids for i in range(len(mds_R))])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.grid()\n",
        "plt.scatter(mds_R[is_object, 0], mds_R[is_object, 1], c='orange', label='object')\n",
        "plt.scatter(mds_R[~is_object, 0], mds_R[~is_object, 1], c='blue', label='not object')\n",
        "plt.legend()\n",
        "plt.xlabel('MDS component 1', fontsize=20)\n",
        "plt.ylabel('MDS component 2', fontsize=20)\n",
        "xlim = plt.gca().get_xlim()\n",
        "ylim = plt.gca().get_ylim()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjIVYAR0fVS4"
      },
      "outputs": [],
      "source": [
        "# @title Plot MDS with images\n",
        "from PIL import ImageOps\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "plt.grid()\n",
        "\n",
        "images = [img_list[i] for i in idxs_test]\n",
        "images = [Image.open(os.path.join(stim_dir, img_name)) for img_name in images]\n",
        "\n",
        "# Function to create an AnnotationBbox with an image and a colored border\n",
        "def image_annotation(ax, xy, image, zoom=0.1, border_color='red'):\n",
        "    # Add a border to the image\n",
        "    border_size = max(1, int(max(image.size) * 0.05))  # Border is 5% of the image size\n",
        "    image_with_border = ImageOps.expand(image, border=border_size, fill=border_color)\n",
        "\n",
        "    imagebox = OffsetImage(image_with_border, zoom=zoom)\n",
        "    ab = AnnotationBbox(imagebox, xy, frameon=False, pad=0.3)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "# Add images to the plot with a colored border\n",
        "for i in range(len(mds_R)):\n",
        "    xy = (mds_R[i, 0], mds_R[i, 1])\n",
        "    border_color = 'blue' if is_object[i] else 'red'\n",
        "    image_annotation(ax, xy, images[i], zoom=0.08 if is_object[i] else 0.08, border_color=border_color)\n",
        "\n",
        "plt.xlim(xlim)\n",
        "plt.ylim(ylim)\n",
        "\n",
        "plt.xlabel('MDS component 1', fontsize=20)\n",
        "plt.ylabel('MDS component 2', fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "####**Question:**\n",
        "Looking at our MDS results, we don't see a very distinct pattern between the two image categories used. Provide **two** reasons why this might be the case in our current analysis."
      ],
      "metadata": {
        "id": "i3VVIzxMKlYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER HERE**"
      ],
      "metadata": {
        "id": "Tp1OSxYkMQpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "The end of part 1\n",
        "___"
      ],
      "metadata": {
        "id": "lVMlt6COMqMn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}